from typing import List
from sentence_transformers import SentenceTransformer
from sentence_transformers import CrossEncoder
from sklearn.metrics.pairwise import cosine_similarity
import os ,json
import time
from pathlib import Path
from mcpServer import mcp  # ç»Ÿä¸€çš„MCPå®ä¾‹ï¼ŒåŒ…å«Bç«™æœç´¢å’ŒModelScope API
import asyncio
import speak

import chromadb

# è®¾ç½®æ›´å¤§çš„ä¸‹è½½é™åˆ¶
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '1200'  # 20åˆ†é’Ÿè¶…æ—¶
os.environ['HF_HUB_MAX_RETRIES'] = '10'        # æœ€å¤§é‡è¯•æ¬¡æ•°
os.environ['HF_HUB_DOWNLOAD_CHUNK_SIZE'] = '1048576'  # 1MBå—å¤§å°
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨é•œåƒæºï¼ˆå¦‚æœå¯ç”¨ï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # ä¾‹å¦‚ä½¿ç”¨é•œåƒæº
'''
ä½¿ç”¨HuggingFaceçš„SentenceTransformeræ¨¡å‹è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–å’Œç›¸ä¼¼åº¦è®¡ç®—
ä¸­æ–‡æ–‡æœ¬ â†’ [model.encode()] â†’ å‘é‡è¡¨ç¤º â†’ [ç›¸ä¼¼åº¦è®¡ç®—] â†’ ç›¸ä¼¼åº¦åˆ†æ•°

å¬å›ï¼šæ ¹æ®æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰æ®µè½å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„æ®µè½
é‡æ’: æ ¹æ®æŸ¥è¯¢å‘é‡å’Œæ®µè½å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„æ®µè½

'''

# chromadb_client = chromadb.EphemeralClient()
chromadb_client = chromadb.PersistentClient(path="./chroma_db") ## æŒä¹…åŒ–
collection = chromadb_client.get_or_create_collection("news")

## æ¨¡å‹åŠ è½½
def load_model_with_retry(model_name="shibing624/text2vec-base-chinese", max_retries=3):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹åŠ è½½å‡½æ•°
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒ
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    
    # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
    cache_path = Path("./cache")
    model_cache_path = cache_path / f"models--{model_name.replace('/', '--')}"
    
    # å¦‚æœæœ¬åœ°ç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥ä»æœ¬åœ°åŠ è½½
    if model_cache_path.exists():
        # è·å–æœ€æ–°çš„å¿«ç…§ç›®å½•
        snapshots_path = model_cache_path / "snapshots"
        if snapshots_path.exists():
            # è·å–å¿«ç…§ç›®å½•ä¸‹çš„ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰ç›®å½•
            snapshot_dirs = [d for d in snapshots_path.iterdir() if d.is_dir()]
            if snapshot_dirs:
                local_model_path = snapshot_dirs[0]
                print(f"ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜ï¼Œä» {local_model_path} åŠ è½½æ¨¡å‹...")
                # print(f"âš ï¸  æ³¨æ„ï¼šè¯·ç¡®ä¿æœ¬åœ°ç¼“å­˜ç›®å½•ä¸‹åªæœ‰ä¸€ä¸ªæ¨¡å‹ç‰ˆæœ¬ï¼:{str(local_model_path)}")
                try:
                    model = SentenceTransformer(
                        str(local_model_path),
                        trust_remote_code=True,
                        cache_folder="./cache",
                    )
                    print("âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹æˆåŠŸï¼")
                    return model
                except Exception as e:
                    print(f"âŒ ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
    
    # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥æˆ–ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä½¿ç”¨è¿œç¨‹åç§°ï¼ˆå¸¦ç¼“å­˜ï¼‰
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•åŠ è½½æ¨¡å‹ (ç¬¬ {attempt + 1} æ¬¡)...")
            
            model = SentenceTransformer(
                model_name,
                trust_remote_code=True,
                cache_folder="./cache",
            )
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return model
            
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            if attempt < max_retries - 1:
                print(f"â³ ç­‰å¾…åé‡è¯•...")
                time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
            else:
                print("ğŸ’¥ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                raise e
## æ–‡æœ¬åˆ†å‰²
def split_into_chunks(doc_file: str) -> List[str]:
    with open(doc_file, 'r', encoding='utf-8') as file:
        content = file.read()

    chunks = [chunk for chunk in content.split("\n\n") if chunk.strip()]
    # for i, chunk in enumerate(chunks):
    #     print(f"[{i}] {chunk}\n")
    print(f"åˆ†æ®µæ•°ï¼š{len(chunks)}")
    return chunks

## ä¿å­˜å‘é‡
def save_embeddings(chunks:List[str], embeddings):

    ids = [str(i) for i in range(len(chunks))]
    
    # Ensure embeddings is in the correct format (list of lists)
    # If we have a single chunk but embeddings is a 1D array, wrap it
    if len(chunks) == 1 and len(embeddings) > 0 and not isinstance(embeddings[0], list):
        embeddings = [embeddings]
    
    collection.add(
        documents=chunks,
        embeddings=embeddings,
        ids=ids
    )
    result = collection.get(ids=ids, include=['embeddings', 'documents'])
    print(f"âœ… å‘é‡ä¿å­˜æˆåŠŸï¼{len(result['embeddings'])}")

 ## å‘é‡æŸ¥è¯¢
def calculate_similarity(query: str, chunks: List[str], model: SentenceTransformer, top_k: int = 5):
    # Encode the query
    query_embedding = model.encode([query])  # Wrap in a list to get 2D array
    # chunk_embeddings = model.encode(chunks)
    chunk_embeddings = collection.get(ids=[str(i) for i in range(len(chunks))], include=['embeddings'])['embeddings']
    
    # Calculate cosine similarity between query and chunks
      # æ ¹æ®æŸ¥è¯¢å‘é‡ä¸æ‰€æœ‰æ®µè½å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„æ®µè½
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
    top_indices = similarities.argsort()[::-1][:top_k]
    return [chunks[i] for i in top_indices]
## å‘é‡å¬å›
def query_embeddings(query: str, chunks: List[str], model: SentenceTransformer, top_k: int = 5) -> List[str]:
    print(f"æŸ¥è¯¢ï¼š{query}")
    query_embedding = model.encode([query])  # Wrap in a list to get 2D array
    # chunk_embeddings = collection.get(ids=[str(i) for i in range(len(chunks))], include=['embeddings'])['embeddings']
    results = collection.query(
        query_embeddings=query_embedding.tolist(),
        n_results=top_k,
        include=['documents', 'embeddings']
    )
    # print(f"æŸ¥è¯¢ç»“æœï¼š{results}")
    return results['documents'][0]

## é‡æ’
def rerank(query: str, retrieved_chunks: List[str], top_k: int) -> List[str]:
    from pathlib import Path
    
    # æ„å»ºæœ¬åœ°æ¨¡å‹è·¯å¾„
    model_name = 'cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'
    cache_path = Path("./cache")
    model_cache_path = cache_path / f"models--{model_name.replace('/', '--')}"
    
    # å¦‚æœæœ¬åœ°ç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥ä»æœ¬åœ°åŠ è½½
    cross_encoder = None
    if model_cache_path.exists():
        # è·å–æœ€æ–°çš„å¿«ç…§ç›®å½•
        snapshots_path = model_cache_path / "snapshots"
        if snapshots_path.exists():
            # è·å–å¿«ç…§ç›®å½•ä¸‹çš„æœ€æ–°ç›®å½•
            snapshot_dirs = [d for d in snapshots_path.iterdir() if d.is_dir()]
            if snapshot_dirs:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„å¿«ç…§
                latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                local_model_path = latest_snapshot
                print(f"ğŸ” æ£€æµ‹åˆ°æœ¬åœ°é‡æ’æ¨¡å‹ç¼“å­˜ï¼Œä» {local_model_path} åŠ è½½...")
                try:
                    cross_encoder = CrossEncoder(
                        str(local_model_path),
                        trust_remote_code=True,
                        cache_folder="./cache"
                    )
                    print("âœ… ä»æœ¬åœ°ç¼“å­˜åŠ è½½é‡æ’æ¨¡å‹æˆåŠŸï¼")
                except Exception as e:
                    print(f"âŒ ä»æœ¬åœ°ç¼“å­˜åŠ è½½é‡æ’æ¨¡å‹å¤±è´¥: {e}")
    
    # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥æˆ–ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä¸‹è½½
    if cross_encoder is None:
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨é•œåƒæºå’Œå¢åŠ ä¸‹è½½è¶…æ—¶
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '1200'  # 20åˆ†é’Ÿè¶…æ—¶
        os.environ['HF_HUB_MAX_RETRIES'] = '10'        # æœ€å¤§é‡è¯•æ¬¡æ•°
        os.environ['HF_HUB_DOWNLOAD_CHUNK_SIZE'] = '1048576'  # 1MBå—å¤§å°
        
        # å¸¦é‡è¯•æœºåˆ¶çš„CrossEncoderåŠ è½½
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"å°è¯•åŠ è½½é‡æ’æ¨¡å‹ (ç¬¬ {attempt + 1} æ¬¡)...")
                cross_encoder = CrossEncoder(model_name, trust_remote_code=True, cache_folder="./cache")
                print("âœ… é‡æ’æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                break
            except Exception as e:
                print(f"âŒ ç¬¬ {attempt + 1} æ¬¡å°è¯•åŠ è½½é‡æ’æ¨¡å‹å¤±è´¥: {e}")
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥äº†
                    print("âš ï¸  é‡æ’æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡é‡æ’æ­¥éª¤ï¼Œç›´æ¥è¿”å›æ£€ç´¢ç»“æœ")
                    return retrieved_chunks[:top_k]  # ç›´æ¥è¿”å›å‰top_kä¸ªæ£€ç´¢ç»“æœ
    
    pairs = [(query, chunk) for chunk in retrieved_chunks]
    scores = cross_encoder.predict(pairs)

    scored_chunks = list(zip(retrieved_chunks, scores))
    scored_chunks.sort(key=lambda x: x[1], reverse=True)
    # print(f"é‡æ’ç»“æœï¼š{scored_chunks}")

    return [chunk for chunk, _ in scored_chunks][:top_k]


## è°ƒç”¨å¤§æ¨¡å‹
async def mcp_chat(query: str, chunks: List[str]):
    print(f"chatæ¨¡å‹è°ƒç”¨ é—®é¢˜ï¼š{query}")
    # æ„å»ºæç¤ºè¯ï¼Œé¿å…f-stringä¸­çš„å¤šè¡Œå­—ç¬¦ä¸²é—®é¢˜
    related_chunks = "\n\n".join(chunks)
    summary_prompt = f'''ä½ æ˜¯ä¸€ä½çŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œä¸‹åˆ—ç‰‡æ®µç”Ÿæˆå‡†ç¡®çš„å›ç­”ã€‚
        ç”¨æˆ·é—®é¢˜: {query}
        ç›¸å…³ç‰‡æ®µ:
        {related_chunks}
        è¯·åŸºäºä¸Šè¿°å†…å®¹ä½œç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚'''
    # await åªèƒ½åœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨ï¼Œæ­¤å¤„éœ€æ”¹ä¸ºåŒæ­¥è°ƒç”¨æˆ–åŒ…è£…ä¸ºå¼‚æ­¥å‡½æ•°
    summary_result = await mcp.call_tool("modelscope_chat_completion", {
        "prompt": summary_prompt,
        "model": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "temperature": 0.7,
        # "max_tokens": 200
    })
    # print(summary_result)
    summary_content = summary_result[0][0].text
    result_dict = json.loads(summary_content) if isinstance(summary_content, str) else summary_content
    print(result_dict)
    content = result_dict.get("content", "") if isinstance(result_dict, dict) else ""
    print(f"  âœ… AIæ€»ç»“å®Œæˆ")
    return content

if __name__ == "__main__":
    chunks = split_into_chunks("news.txt")
    model = load_model_with_retry()
    
    embeddings = model.encode(chunks).tolist()
    save_embeddings(chunks, embeddings)

    query = "ç¾å›½çš„æˆ˜ç•¥"
    results = query_embeddings(query, chunks, model, top_k=100)
    # similarities = calculate_similarity(query, results, model, top_k=100)   
    # print(f"ç›¸ä¼¼åº¦ï¼š{similarities}")

    # print(f"é‡æ’å‰{results}")
    results = rerank(query, results, top_k=5)
    # print(f"é‡æ’å{results}")
    for i, result in enumerate(results):
        print(f"[{i}] {result}\n")

    # summary_content = asyncio.run(mcp_chat(query, chunks))
    # print(summary_content)
    # with open("summary.md", "w", encoding="utf-8") as f:
    #     f.write(summary_content)
    # asyncio.run(speak.generate_speech(file_path="summary.md", output_file="summary.mp3"))
